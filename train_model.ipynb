{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.2243 - acc: 0.9283 - val_loss: 0.0400 - val_acc: 0.9869\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.0291 - val_acc: 0.9902\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0262 - val_acc: 0.9909\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0192 - acc: 0.9941 - val_loss: 0.0240 - val_acc: 0.9920\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0132 - acc: 0.9959 - val_loss: 0.0300 - val_acc: 0.9914\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.0208 - val_acc: 0.9931\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0074 - acc: 0.9976 - val_loss: 0.0217 - val_acc: 0.9934\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0222 - val_acc: 0.9934\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0249 - val_acc: 0.9938\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0301 - val_acc: 0.9926\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0310 - val_acc: 0.9927\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0330 - val_acc: 0.9932\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0326 - val_acc: 0.9935\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 7.6342e-04 - acc: 0.9999 - val_loss: 0.0318 - val_acc: 0.9929\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 3.2514e-04 - acc: 1.0000 - val_loss: 0.0316 - val_acc: 0.9937\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 3.0674e-04 - acc: 1.0000 - val_loss: 0.0329 - val_acc: 0.9934\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 2.9095e-04 - acc: 1.0000 - val_loss: 0.0330 - val_acc: 0.9936\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 2.8534e-04 - acc: 1.0000 - val_loss: 0.0338 - val_acc: 0.9936\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 2.8238e-04 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 0.9936\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 2.7980e-04 - acc: 1.0000 - val_loss: 0.0347 - val_acc: 0.9936\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 4,575,242\n",
      "Trainable params: 4,575,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "MODEL_NAME = '1608_mnist'\n",
    "\n",
    "''' \n",
    "loads mnist data \n",
    "reshapes train and test data\n",
    "converts their types to float\n",
    "divides them by 255 to scale data to fit between 0 and 1\n",
    "converts to one hot encoding\n",
    "returns data and labels for both training and test\n",
    "'''\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    y_test = keras.utils.to_categorical(y_test, 10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "'''\n",
    "Creates the structure of our model\n",
    "'''\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu', \\\n",
    "            input_shape=[28, 28, 1]))\n",
    "    # 28*28*64\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 14*14*64\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    # 14*14*128\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 7*7*128\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, strides=1, \\\n",
    "            padding='same', activation='relu'))\n",
    "    # 7*7*256\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "    # 4*4*256\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "'''\n",
    "trains our model with given data\n",
    "\n",
    "categorical-cross-entropy\n",
    "Adadelta\n",
    "20 epochs\n",
    "128 batch size\n",
    "'''\n",
    "def train(model, x_train, y_train, x_test, y_test):\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \\\n",
    "                  optimizer=keras.optimizers.Adadelta(), \\\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, \\\n",
    "              batch_size=128, \\\n",
    "              epochs=20, \\\n",
    "              verbose=1, \\\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "'''\n",
    "Freezes model variables, saves the model as protobuf file\n",
    "So that it is usable in Android devices.\n",
    "'''\n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "\n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n",
    "\n",
    "\n",
    "if not path.exists('out'):\n",
    "    os.mkdir('out')\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "train(model, x_train, y_train, x_test, y_test)\n",
    "\n",
    "'''\n",
    "While saving our model, we need to specify input node and output node. Therefore before\n",
    "calling export_model method, we need to summarize our model to see the names of these nodes\n",
    "'''\n",
    "print(model.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from out/1608_mnist_convnet.chkp\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "graph saved!\n"
     ]
    }
   ],
   "source": [
    " export_model(tf.train.Saver(), model, [\"conv2d_7_input\"], \"dense_6/Softmax\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
